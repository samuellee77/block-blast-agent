import os
import time
import numpy as np

from stable_baselines3 import DQN
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.monitor import Monitor

from blockblast_game.game_env import BlockGameEnv
from agents.agent_visualizer import visualize_agent

# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
MODELS_DIR = os.path.join(SCRIPT_DIR, "models")
LOGS_DIR = os.path.join(SCRIPT_DIR, "logs")
os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(LOGS_DIR, exist_ok=True)


class MaskableDQN(DQN):
    def predict(
        self,
        observation,
        state=None,
        episode_start=None,
        deterministic: bool = False,
        mask=None,
        **kwargs,
    ):
        # 1) Raw Q-values
        obs_tensor, _ = self.policy.obs_to_tensor(observation)
        q_values = self.q_net(obs_tensor).cpu().data.numpy()  # (batch_size, n_actions)

        # 2) Build masks from each sub-env
        masks = np.array([env.env.action_masks() for env in self.env.envs])

        # 3) Invalidate impossible actions
        q_values[~masks] = -1e8

        # 4) Greedy actions
        greedy = q_values.argmax(axis=1)

        if deterministic:
            # inference mode: unwrap to scalar if single-env
            if greedy.shape[0] == 1:
                return int(greedy[0]), None
            return greedy, None

        # 5) Îµ-greedy for training: always return an array
        batch_size, _ = q_values.shape
        actions = greedy.copy()
        for i in range(batch_size):
            if np.random.rand() < self.exploration_rate:
                valid = np.nonzero(masks[i])[0]
                actions[i] = np.random.choice(valid)

        # during training, ALWAYS return an array of shape (n_envs,)
        return actions, None


def make_env(rank: int, seed: int = 0):
    """
    Create one Monitor-wrapped BlockGameEnv.
    We'll vectorize with DummyVecEnv, then patch sample() on the VecEnv.
    """

    def _init():
        env = BlockGameEnv()
        env = Monitor(env)
        try:
            env.reset(seed=seed + rank)
        except TypeError:
            env.seed(seed + rank)
        return env

    return _init


def train_masked_dqn(total_timesteps: int = 500_000, continue_training: bool = False):
    # 1) Build a single-env VecEnv
    env = DummyVecEnv([make_env(0)])

    # 2) Patch the VecEnv's action_space.sample() so warm-up only sees valid moves
    env.action_space.sample = lambda: int(
        np.random.choice(np.nonzero(env.envs[0].env.action_masks())[0])
    )

    # 3) Checkpoint callback
    checkpoint_cb = CheckpointCallback(
        save_freq=100_000,
        save_path=MODELS_DIR,
        name_prefix="masked_dqn_model",
        save_replay_buffer=False,
        save_vecnormalize=True,
    )

    # 4) Load or create the MaskableDQN
    final_zip = os.path.join(MODELS_DIR, "final_masked_dqn_model.zip")
    if continue_training and os.path.isfile(final_zip):
        print("[masked_dqn] Continuing from existing model")
        model = MaskableDQN.load(final_zip, env=env)
    else:
        model = MaskableDQN(
            "MultiInputPolicy",
            env,
            verbose=1,
            tensorboard_log=LOGS_DIR,
            learning_rate=1e-4,
            buffer_size=1_000_000,
            learning_starts=20_000,
            batch_size=128,
            gamma=0.99,
            tau=0.005,
            target_update_interval=500,
            exploration_fraction=0.05,
            exploration_initial_eps=0.5,
            exploration_final_eps=0.03,
            train_freq=(4, "step"),
            gradient_steps=2,
        )

    # 5) Train
    print(f"[masked_dqn] Training for {total_timesteps} timesteps...")
    start = time.time()
    model.learn(total_timesteps=total_timesteps, callback=checkpoint_cb)
    print(f"[masked_dqn] Done in {(time.time() - start):.1f}s")

    # 6) Save
    model.save(os.path.join(MODELS_DIR, "final_masked_dqn_model"))
    return model


if __name__ == "__main__":
    total_timesteps = 20_000_000
    continue_training = False
    do_train = True
    do_visualize = True

    if do_train:
        train_masked_dqn(total_timesteps, continue_training)

    if do_visualize:
        # Build a render env and patch its sample() as well
        render_env = Monitor(BlockGameEnv(render_mode="human"))
        render_env.action_space.sample = lambda: int(
            np.random.choice(np.nonzero(render_env.env.action_masks())[0])
        )

        model_file = os.path.join(MODELS_DIR, "final_masked_dqn_model.zip")
        print(f"[masked_dqn] Loading model from {model_file}")
        loaded = MaskableDQN.load(model_file, env=render_env)
        visualize_agent(
            render_env,
            loaded,
            episodes=10,
            delay=0.2,
            use_masks=True,
            window_title="Maskable DQN Agent",
        )
